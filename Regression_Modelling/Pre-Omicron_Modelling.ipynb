{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d54189d",
   "metadata": {},
   "source": [
    "# Preprocessing and Modelling Pre-Omicron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58c067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore, rankdata, kendalltau\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import csv \n",
    "from collections import Counter\n",
    "import datetime\n",
    "import holidays\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "\n",
    "from lineartree import LinearTreeRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f372efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        72091.0\n",
       "1       122994.0\n",
       "2       117865.0\n",
       "3       151282.0\n",
       "4       131881.0\n",
       "          ...   \n",
       "1479     46435.0\n",
       "1480     33636.0\n",
       "1481     29986.0\n",
       "1482     74748.0\n",
       "1483    240219.0\n",
       "Name: School-Aged Population, Length: 1484, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When I did my first modelling, I did not ensure I had the proper data type for all columns; this\n",
    "# resulted in \"Test Date\" (previously encoded as an object) being assigned to cat_cols and subsquently onehot encoded,\n",
    "# inflating my scores. This is resolved below. Scores were previously about 10 percentage points higher on simple\n",
    "# linear models.\n",
    "merged_df_pre_omi = pd.read_csv('data/pre_omi.csv')\n",
    "\n",
    "merged_df_pre_omi['Test Date'] = pd.to_datetime(merged_df_pre_omi['Test Date'])\n",
    "\n",
    "merged_df_pre_omi['School Status'] = merged_df_pre_omi['School Status'].astype('object')\n",
    "merged_df_pre_omi['School-Aged Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b24ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Capita Gene Copies\n",
      "Population Served, estimated\n",
      "School-Aged Population\n",
      "Holiday\n"
     ]
    }
   ],
   "source": [
    "# Simple preprocessing. Not going to worry about scaling right now. Just attempting to determine if my data is\n",
    "# strong enough to warrant continuing.\n",
    "\n",
    "cat_cols = []\n",
    "\n",
    "for i in merged_df_pre_omi.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Test Date':\n",
    "        pass\n",
    "    elif merged_df_pre_omi[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif merged_df_pre_omi[i].dtype == 'float64' or 'int64':\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "\n",
    "cat_transformer = Pipeline(steps=[  \n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))                     \n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols),\n",
    "    ])\n",
    "\n",
    "y = merged_df_pre_omi['Gene Copies (N1/L)']\n",
    "\n",
    "X = merged_df_pre_omi.drop('Gene Copies (N1/L)', axis=1)\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220b8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up columns to be onehot encoded. We will look at numerical transformation later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Have to drop the NaNs, otherwise linear model won't work\n",
    "y_train = y_train.dropna()\n",
    "X_train = X_train.dropna()\n",
    "X_test = X_test.dropna()\n",
    "y_test = y_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2575b6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5587991188685084"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred=linreg.predict(X_test)\n",
    "\n",
    "linreg.score(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29cea349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Date: -1070.2982270060952\n",
      "WRRF Name: 1000.1580037146778\n",
      "Per Capita Gene Copies: 1133.043333951316\n",
      "Population Served, estimated: -729.9289403163428\n",
      "School Status: 3534.929886399773\n",
      "School-Aged Population: -1595.292150257837\n",
      "Season: -2604.654484580207\n",
      "Holiday: 1246.104726306294\n"
     ]
    }
   ],
   "source": [
    "linear_model = linreg.named_steps['model']\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, coef in zip(X.columns, linear_model.coef_):\n",
    "    print(f\"{feature_name}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16865a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try again but with log-transformed targets\n",
    "y_train_2 = np.log(y_train)\n",
    "y_test_2 = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "661a9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631971730326365"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_2.fit(X_train, y_train_2)\n",
    "\n",
    "y_pred_2=linreg_2.predict(X_test)\n",
    "\n",
    "linreg_2.score(X_train,y_train_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035362af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Date: -0.0440007780691526\n",
      "WRRF Name: 0.20732123218866916\n",
      "Per Capita Gene Copies: 0.19162232107698013\n",
      "Population Served, estimated: -0.14624733932704054\n",
      "School Status: 0.31736815887438546\n",
      "School-Aged Population: -0.1297000508508879\n",
      "Season: -0.3831365358061128\n",
      "Holiday: 0.4055824921338149\n"
     ]
    }
   ],
   "source": [
    "linear_model = linreg_2.named_steps['model']\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, coef in zip(X.columns, linear_model.coef_):\n",
    "    print(f\"{feature_name}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7f98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the addition of jewish holidays, got a 1% boost in the r2 score. Might get a little nudge with islamic, but\n",
    "# probably not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "836cb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying without the highly-correlated features\n",
    "\n",
    "copy_df = merged_df_pre_omi.copy()\n",
    "\n",
    "copy_df.dropna(inplace=True)\n",
    "\n",
    "y = copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "\n",
    "X = copy_df.drop(columns=['Gene Copies (N1/L)','Population Served, estimated', 'Per Capita Gene Copies'], axis=1)\n",
    "\n",
    "# Split again\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# Log transforming the target\n",
    "\n",
    "y_test = np.log(y_test)\n",
    "y_train = np.log(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "359e13a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6177258476117596"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd iteration, no adjustments to the model itself\n",
    "\n",
    "linreg_3 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_3.predict(X_test)\n",
    "\n",
    "linreg_3.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb1bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Date: -0.14757150645203634\n",
      "WRRF Name: 0.17730798873563733\n",
      "School Status: 0.21434747417862648\n",
      "School-Aged Population: -0.1206607271795389\n",
      "Season: 0.28797817715926266\n",
      "Holiday: -0.1298456319366709\n"
     ]
    }
   ],
   "source": [
    "linear_model = linreg_3.named_steps['model']\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, coef in zip(X.columns, linear_model.coef_):\n",
    "    print(f\"{feature_name}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b70af9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly worse score without the non-school population data. So even though it's correlated, the general population\n",
    "# data is not boosting the score much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4773f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just the school-related data.\n",
    "\n",
    "copy_df_2 = merged_df_pre_omi.copy()\n",
    "copy_df_2.dropna(inplace=True)\n",
    "\n",
    "y = copy_df_2['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_2[['School-Aged Population', 'School Status']]\n",
    "\n",
    "\n",
    "# We only have one column to transform in this version\n",
    "\n",
    "cat_cols_2 = ['School Status']\n",
    "\n",
    "preprocessor_2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols_2),\n",
    "    ])\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05cb26cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46997170371988195"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4th model - this one is actually different!\n",
    "\n",
    "linreg_4 = Pipeline([\n",
    "    ('preprocessor', preprocessor_2),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_4.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_4.predict(X_test)\n",
    "\n",
    "linreg_4.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ee16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so with just the school-aged population and school status, our model explains nearly half the variance! And this\n",
    "# is before log transformation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e9d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5422711234817856\n",
      "0.5143954687950946\n"
     ]
    }
   ],
   "source": [
    "# Same model, log-transformed target data.\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "linreg_5 = Pipeline([\n",
    "    ('preprocessor', preprocessor_2),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_5.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_5.predict(X_test)\n",
    "\n",
    "print(linreg_5.score(X_train,y_train))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5bebc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, our train tests scores are pretty close! Not too much overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd54dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New X and y\n",
    "\n",
    "copy_df_3 = merged_df_pre_omi.copy()\n",
    "copy_df_3.dropna(inplace=True)\n",
    "y = copy_df_3['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_3['School-Aged Population']\n",
    "\n",
    "\n",
    "# Split again\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# log transform again\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6996df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     Gene Copies (N1/L)   R-squared:                       0.007\n",
      "Model:                            OLS   Adj. R-squared:                  0.007\n",
      "Method:                 Least Squares   F-statistic:                     8.238\n",
      "Date:                Sun, 30 Jul 2023   Prob (F-statistic):            0.00418\n",
      "Time:                        17:18:11   Log-Likelihood:                -1784.3\n",
      "No. Observations:                1097   AIC:                             3573.\n",
      "Df Residuals:                    1095   BIC:                             3583.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                      8.1025      0.074    108.769      0.000       7.956       8.249\n",
      "School-Aged Population  1.787e-06   6.23e-07      2.870      0.004    5.66e-07    3.01e-06\n",
      "==============================================================================\n",
      "Omnibus:                       63.016   Durbin-Watson:                   2.036\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               39.333\n",
      "Skew:                          -0.331   Prob(JB):                     2.88e-09\n",
      "Kurtosis:                       2.350   Cond. No.                     2.40e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.4e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "602     1.749047\n",
      "313    -0.568979\n",
      "1420   -0.871085\n",
      "737     0.765110\n",
      "818    -0.498675\n",
      "          ...   \n",
      "1198    1.493369\n",
      "1238   -0.735291\n",
      "1405    0.034741\n",
      "934    -0.292939\n",
      "1234   -1.714324\n",
      "Length: 1097, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# using ols because we are not preprocessing this data\n",
    "\n",
    "\n",
    "X_int = sm.add_constant(X_train)\n",
    "results = sm.OLS(y_train, X_int).fit()\n",
    "summary = results.summary()\n",
    "print(summary)\n",
    "\n",
    "influence = OLSInfluence(results)\n",
    "print(influence.resid_studentized)\n",
    "\n",
    "# well, this isn't very good! This is just school-aged population, though, and not the actual status of schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "201df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wonder how predictive the baseline model is. Let's just look at the original population data, which is one of\n",
    "# the most relevant features. \n",
    "\n",
    "copy_df_4 = merged_df_pre_omi.copy()\n",
    "copy_df_4.dropna(inplace=True)\n",
    "\n",
    "\n",
    "y = copy_df_4['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_4['Population Served, estimated']\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0bd2bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>Gene Copies (N1/L)</td> <th>  R-squared:         </th> <td>   0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                    <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>              <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   11.33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Sun, 30 Jul 2023</td>  <th>  Prob (F-statistic):</th> <td>0.000791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>17:18:12</td>      <th>  Log-Likelihood:    </th> <td> -1782.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>  1097</td>       <th>  AIC:               </th> <td>   3569.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>  1095</td>       <th>  BIC:               </th> <td>   3579.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                        <td>    8.0670</td> <td>    0.075</td> <td>  106.995</td> <td> 0.000</td> <td>    7.919</td> <td>    8.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Population Served, estimated</th> <td> 3.618e-07</td> <td> 1.08e-07</td> <td>    3.365</td> <td> 0.001</td> <td> 1.51e-07</td> <td> 5.73e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>61.143</td> <th>  Durbin-Watson:     </th> <td>   2.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  38.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.330</td> <th>  Prob(JB):          </th> <td>3.98e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.359</td> <th>  Cond. No.          </th> <td>1.42e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.42e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:     Gene Copies (N1/L)   R-squared:                       0.010\n",
       "Model:                            OLS   Adj. R-squared:                  0.009\n",
       "Method:                 Least Squares   F-statistic:                     11.33\n",
       "Date:                Sun, 30 Jul 2023   Prob (F-statistic):           0.000791\n",
       "Time:                        17:18:12   Log-Likelihood:                -1782.7\n",
       "No. Observations:                1097   AIC:                             3569.\n",
       "Df Residuals:                    1095   BIC:                             3579.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================================\n",
       "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------\n",
       "const                            8.0670      0.075    106.995      0.000       7.919       8.215\n",
       "Population Served, estimated  3.618e-07   1.08e-07      3.365      0.001    1.51e-07    5.73e-07\n",
       "==============================================================================\n",
       "Omnibus:                       61.143   Durbin-Watson:                   2.034\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               38.686\n",
       "Skew:                          -0.330   Prob(JB):                     3.98e-09\n",
       "Kurtosis:                       2.359   Cond. No.                     1.42e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.42e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_int = sm.add_constant(X_train)\n",
    "model_2 = sm.OLS(y_train, X_int).fit()\n",
    "summary = model_2.summary()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a09e687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df_5 = merged_df_pre_omi.copy()\n",
    "copy_df_5.reset_index()\n",
    "copy_df_5.dropna(inplace=True)\n",
    "y = copy_df_5['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_5[['Population Served, estimated', 'WRRF Name', 'Per Capita Gene Copies', 'Test Date']]\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa973bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07123753415992828\n",
      "0.04505102157812135\n"
     ]
    }
   ],
   "source": [
    "cat_cols_3 = ['WRRF Name']\n",
    "\n",
    "cat_transformer = Pipeline(steps=[  \n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))                     \n",
    "])\n",
    "\n",
    "preprocessor_3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols_3),\n",
    "    ])\n",
    "\n",
    "linreg_6 = Pipeline([\n",
    "    ('preprocessor', preprocessor_3),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_6.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_6.predict(X_test)\n",
    "\n",
    "print(linreg_6.score(X_train,y_train))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36968087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So yeah, the basic info from the original dataset explains almost nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d39e1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Test Date</th>\n",
       "      <th>WRRF Name</th>\n",
       "      <th>Gene Copies (N1/L)</th>\n",
       "      <th>Per Capita Gene Copies</th>\n",
       "      <th>Population Served, estimated</th>\n",
       "      <th>School Status</th>\n",
       "      <th>School-Aged Population</th>\n",
       "      <th>Season</th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>26th Ward</td>\n",
       "      <td>389.0</td>\n",
       "      <td>263535.64</td>\n",
       "      <td>290608</td>\n",
       "      <td>Summer Break_1</td>\n",
       "      <td>72091.0</td>\n",
       "      <td>Summer 2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>Bowery Bay</td>\n",
       "      <td>1204.0</td>\n",
       "      <td>443632.86</td>\n",
       "      <td>924695</td>\n",
       "      <td>Summer Break_1</td>\n",
       "      <td>122994.0</td>\n",
       "      <td>Summer 2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>Coney Island</td>\n",
       "      <td>304.0</td>\n",
       "      <td>168551.56</td>\n",
       "      <td>682342</td>\n",
       "      <td>Summer Break_1</td>\n",
       "      <td>117865.0</td>\n",
       "      <td>Summer 2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>Hunts Point</td>\n",
       "      <td>940.0</td>\n",
       "      <td>574446.57</td>\n",
       "      <td>755948</td>\n",
       "      <td>Summer Break_1</td>\n",
       "      <td>151282.0</td>\n",
       "      <td>Summer 2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>632.0</td>\n",
       "      <td>233077.74</td>\n",
       "      <td>748737</td>\n",
       "      <td>Summer Break_1</td>\n",
       "      <td>131881.0</td>\n",
       "      <td>Summer 2020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>1479</td>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>Port Richmond</td>\n",
       "      <td>881.0</td>\n",
       "      <td>560091.38</td>\n",
       "      <td>226167</td>\n",
       "      <td>Mandatory_In-person</td>\n",
       "      <td>46435.0</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>1480</td>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>Red Hook</td>\n",
       "      <td>250.0</td>\n",
       "      <td>392726.56</td>\n",
       "      <td>224029</td>\n",
       "      <td>Mandatory_In-person</td>\n",
       "      <td>33636.0</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>1481</td>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>Rockaway</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>991360.72</td>\n",
       "      <td>120539</td>\n",
       "      <td>Mandatory_In-person</td>\n",
       "      <td>29986.0</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>1482</td>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>Tallman Island</td>\n",
       "      <td>336.0</td>\n",
       "      <td>387871.66</td>\n",
       "      <td>449907</td>\n",
       "      <td>Mandatory_In-person</td>\n",
       "      <td>74748.0</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>1483</td>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>Wards Island</td>\n",
       "      <td>730.0</td>\n",
       "      <td>1009746.91</td>\n",
       "      <td>1201485</td>\n",
       "      <td>Mandatory_In-person</td>\n",
       "      <td>240219.0</td>\n",
       "      <td>Fall 2021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1484 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Test Date       WRRF Name  Gene Copies (N1/L)  \\\n",
       "0         0 2020-09-01       26th Ward               389.0   \n",
       "1         1 2020-09-01      Bowery Bay              1204.0   \n",
       "2         2 2020-09-01    Coney Island               304.0   \n",
       "3         3 2020-09-01     Hunts Point               940.0   \n",
       "4         4 2020-09-01     Jamaica Bay               632.0   \n",
       "...     ...        ...             ...                 ...   \n",
       "1479   1479 2021-10-27   Port Richmond               881.0   \n",
       "1480   1480 2021-10-27        Red Hook               250.0   \n",
       "1481   1481 2021-10-27        Rockaway              1052.0   \n",
       "1482   1482 2021-10-27  Tallman Island               336.0   \n",
       "1483   1483 2021-10-27    Wards Island               730.0   \n",
       "\n",
       "      Per Capita Gene Copies  Population Served, estimated  \\\n",
       "0                  263535.64                        290608   \n",
       "1                  443632.86                        924695   \n",
       "2                  168551.56                        682342   \n",
       "3                  574446.57                        755948   \n",
       "4                  233077.74                        748737   \n",
       "...                      ...                           ...   \n",
       "1479               560091.38                        226167   \n",
       "1480               392726.56                        224029   \n",
       "1481               991360.72                        120539   \n",
       "1482               387871.66                        449907   \n",
       "1483              1009746.91                       1201485   \n",
       "\n",
       "            School Status  School-Aged Population       Season  Holiday  \n",
       "0          Summer Break_1                 72091.0  Summer 2020        0  \n",
       "1          Summer Break_1                122994.0  Summer 2020        0  \n",
       "2          Summer Break_1                117865.0  Summer 2020        0  \n",
       "3          Summer Break_1                151282.0  Summer 2020        0  \n",
       "4          Summer Break_1                131881.0  Summer 2020        0  \n",
       "...                   ...                     ...          ...      ...  \n",
       "1479  Mandatory_In-person                 46435.0    Fall 2021        0  \n",
       "1480  Mandatory_In-person                 33636.0    Fall 2021        0  \n",
       "1481  Mandatory_In-person                 29986.0    Fall 2021        0  \n",
       "1482  Mandatory_In-person                 74748.0    Fall 2021        0  \n",
       "1483  Mandatory_In-person                240219.0    Fall 2021        0  \n",
       "\n",
       "[1484 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to use the index (sample date) as a feature. Let's also save this data file as a csv, since\n",
    "# it's what we ultimately want to use.\n",
    "\n",
    "sample_date_df = merged_df_pre_omi.copy()\n",
    "\n",
    "# sample_date_df.to_csv('data/master_wastewater.csv', index=False)\n",
    "\n",
    "sample_date_df = sample_date_df.reset_index()\n",
    "sample_date_df\n",
    "\n",
    "#(Ending up abandoning dates at features, but in future work would like to integrate them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f040b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test Date']\n",
      "['WRRF Name', 'School Status', 'Season']\n",
      "['index', 'Per Capita Gene Copies', 'Population Served, estimated', 'School-Aged Population']\n"
     ]
    }
   ],
   "source": [
    "# Fancier models! Going to move on to more elaborate models, including Random Forest.\n",
    "\n",
    "# Re-doing the columns for preprocessing since we have different features\n",
    "\n",
    "date_cols = []\n",
    "cat_cols = []  \n",
    "num_cols = []   \n",
    "\n",
    "for i in sample_date_df.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Holiday':\n",
    "        pass\n",
    "    elif sample_date_df[i].dtype == 'datetime64[ns]':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif i == 'Sample Date':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'float64' or sample_date_df[i].dtype == 'int64':\n",
    "        num_cols.append(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "\n",
    "\n",
    "print(date_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)\n",
    "# leaving \"holiday\" out because we don't want to transform this binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cd2084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for fit and transformation of linear features in a pipeline. Were originally trying to combine\n",
    "# LinearRegressor with RandomForest using FeatureUnion, but was unsuccessful\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "173f31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New X and y, split\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop('Gene Copies (N1/L)', axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a6da72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New preprocessors\n",
    "\n",
    "cat_preprocessor = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_preprocessor = Pipeline([\n",
    "    ('selector', ColumnSelector(columns=num_cols)), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_cols),\n",
    "    ('num', numeric_preprocessor, num_cols),\n",
    "])\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  RandomForestRegressor(random_state = 42))\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "967c50c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.9956072253052803\n",
      "R2 test score: 0.9710455736297898\n",
      "Mean squared error: 0.046502273870293584\n",
      "Mean absolute error: 0.1557950212544598\n",
      "Mean absolute percentage error: 0.019436302423708574\n",
      "Explained variance score (modified R2): 0.9711137033002152\n"
     ]
    }
   ],
   "source": [
    "# Just going with a vanilla Random Forest model after trying to combine a linear regression and random forest.\n",
    "\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {rf_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f82ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow, 99.5%! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc5f936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0032408855024702197: index\n",
      "0.0012653541334829968: Holiday\n",
      "0.0012030764686354338: Test Date\n",
      "0.0009657866936284619: School-Aged Population\n",
      "0.0009570865532883691: Season\n",
      "0.0007299278255275428: Population Served, estimated\n",
      "0.0005533291436602843: Per Capita Gene Copies\n",
      "0.0003931522738028303: WRRF Name\n",
      "0.00018733503013810266: School Status\n"
     ]
    }
   ],
   "source": [
    "# Let's look at feature importances:\n",
    "\n",
    "rf_model = rf_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, rf_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1ade2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.9377737 , 0.91335988, 1.04615211, 0.95964003, 0.92230296]),\n",
       " 'score_time': array([0.01658916, 0.01693606, 0.01571393, 0.01702785, 0.01662803]),\n",
       " 'test_score': array([0.97500962, 0.96817421, 0.96257558, 0.9672737 , 0.95461637]),\n",
       " 'train_score': array([0.99503409, 0.99527155, 0.99554904, 0.99514594, 0.99563054])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validating, because we ultimately want to use this on other data!\n",
    "cross_validate(rf_pipeline, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be4f4846",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Sample Date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m sample_copy_df_2 \u001b[38;5;241m=\u001b[39m sample_date_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m sample_copy_df_2\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m sample_copy_df_2\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGene Copies (N1/L)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample Date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Date\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m sample_copy_df_2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGene Copies (N1/L)\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m X_train,X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.20\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MCMPrime/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MCMPrime/lib/python3.11/site-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5400\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5401\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5402\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5403\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5404\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5405\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5406\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5407\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MCMPrime/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MCMPrime/lib/python3.11/site-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MCMPrime/lib/python3.11/site-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MCMPrime/lib/python3.11/site-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Sample Date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Want to try this model again without the date features, since they seem to be over-determining.\n",
    "\n",
    "sample_copy_df_2 = sample_date_df.copy()\n",
    "\n",
    "sample_copy_df_2.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df_2.drop(['Gene Copies (N1/L)', 'Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df_2['Gene Copies (N1/L)']\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.fit(X_train, y_train)  # Same pipeline, different data\n",
    "\n",
    "\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'R-squared score on training data: {rf_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The very same! Our engineered features are very strong all around. Let's look at feature importance.\n",
    "\n",
    "rf_model = rf_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, rf_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36730e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try another model type, using all features\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "X = sample_copy_df.drop('Gene Copies (N1/L)', axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "y.dropna(inplace=True)\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e83afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grad = GradientBoostingRegressor()\n",
    "grad_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  grad)\n",
    "])\n",
    "\n",
    "\n",
    "grad_pipeline.fit(X_train, y_train)\n",
    "y_pred = grad_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {grad_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae966026",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_model = grad_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, grad_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very different weighing of features here, and slightly better test score. Let's look at loss scores, too:\n",
    "train_score = grad_pipeline['model'].train_score_\n",
    "train_score\n",
    "\n",
    "\n",
    "# If we ran maybe double the iteratations, our loss score should approach .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61753617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get rid of all date features with this model and see what happens\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "y.dropna(inplace=True)\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "grad = GradientBoostingRegressor()\n",
    "grad_best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  grad)\n",
    "])\n",
    "\n",
    "grad_best_pipeline.fit(X_train, y_train)\n",
    "y_pred = grad_best_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {grad_best_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135bd58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very similar scores, with less over-fitting. Good! # Let's examine feature importance.\n",
    "\n",
    "grad_model = grad_best_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, grad_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to remove highly-correlated non-engineered features, to better see how strong the model is with my contibutions.\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date', 'Population Served, estimated', 'Per Capita Gene Copies'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do preprocesing as we have different features\n",
    "date_cols = []\n",
    "cat_cols = []  \n",
    "num_cols = []   \n",
    "\n",
    "for i in sample_date_df.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Holiday' or i == 'Population Served, estimated' or i == 'Per Capita Gene Copies':\n",
    "        pass\n",
    "    elif sample_date_df[i].dtype == 'datetime64[ns]':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif i == 'Sample Date':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'float64' or sample_date_df[i].dtype == 'int64':\n",
    "        num_cols.append(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "\n",
    "\n",
    "print(date_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)\n",
    "# leaving \"holiday\" out because we don't want to transform this binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fef3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer = Pipeline(steps=[  \n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))                     \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols),\n",
    "    ])\n",
    "\n",
    "grad = GradientBoostingRegressor()\n",
    "grad_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  grad)\n",
    "])\n",
    "\n",
    "grad_pipeline.fit(X_train, y_train)\n",
    "y_pred = grad_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {grad_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at feature importances\n",
    "\n",
    "grad_model = grad_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, grad_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ba988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By removing correlated features, our score was reduced pretty dramatically. However, there seem to be important\n",
    "# elements of the non-date correlated features that we should keep, because although they are similar, they tell us\n",
    "# important things about the data at particular times. For instance, sudden wastewater spikes around school events\n",
    "# in places with high school-aged populations. Over time, these effects are expected to flatten out and/or become\n",
    "# more cyclical. One avenue to pursue later would be population estimates of those who left the city during 2020/2021\n",
    "# but who were never officially non-residents, and thus not reflected in the Census Bureau's data. We would expect much more of this in wealthier zip codes and in zip codes\n",
    "# where there are fewer children. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6992721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up for grid search of our best model so far. Re-doing preprocessing since using different features\n",
    "\n",
    "date_cols = []\n",
    "cat_cols = []  \n",
    "num_cols = []   \n",
    "\n",
    "for i in sample_date_df.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Holiday':\n",
    "        pass\n",
    "    elif sample_date_df[i].dtype == 'datetime64[ns]':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif i == 'Sample Date':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'float64' or sample_date_df[i].dtype == 'int64':\n",
    "        num_cols.append(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "print(date_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)\n",
    "\n",
    "cat_preprocessor = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_preprocessor = Pipeline([\n",
    "    ('selector', ColumnSelector(columns=num_cols)), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_cols),\n",
    "    ('num', numeric_preprocessor, num_cols),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ddb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the datetime info and re-running grid search on our best model.\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "grad = GradientBoostingRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [.01, .1, .3],\n",
    "    'model__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "    'model__n_estimators': [100, 150, 200, 250],\n",
    "    'model__subsample': [.3, .5, .7, 1.0],\n",
    "    'model__criterion': ['friedman_mse', 'squared_error']\n",
    "    \n",
    "}\n",
    "                            \n",
    "grid_search_gbc = GridSearchCV(\n",
    "    estimator = grad_best_pipeline,  # pipeline \n",
    "    param_grid = param_grid,\n",
    "    cv= 5,\n",
    "    scoring='explained_variance'  # internal scoring term\n",
    ")\n",
    "\n",
    "grid_search_gbc.fit(X_train, y_train)\n",
    "\n",
    "cv_score = grid_search_gbc.best_score_\n",
    "test_score = r2_score(y_test, grid_search_gbc.predict(X_test))\n",
    "\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n",
    "print(grid_search_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5927df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thought I was done, but want to try one more model type: LinearTrees!\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b18a8d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.9802052054633759\n",
      "R2 test score: -1184.0720230715035\n",
      "Mean squared error: 1903.285634751599\n",
      "Mean absolute error: 3.5056331972161354\n",
      "Mean absolute percentage error: 0.4220202108653636\n",
      "Explained variance score (modified R2 test score): -1177.0278641925847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lintree = LinearTreeRegressor(base_estimator=LinearRegression())\n",
    "\n",
    "lintree_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('to_sparse', FunctionTransformer(csr_matrix, validate=False)),\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.toarray(), validate=False)),\n",
    "    ('model',  lintree)\n",
    "])\n",
    "\n",
    "lintree_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lintree_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {lintree_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')\n",
    "\n",
    "# Well, that was a terrible model for this data, but seemed interesting! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MCMPrime)",
   "language": "python",
   "name": "mcmprime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
