{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d54189d",
   "metadata": {},
   "source": [
    "# Preprocessing and Modelling Pre-Omicron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58c067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import csv \n",
    "from collections import Counter\n",
    "import datetime\n",
    "import holidays\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "\n",
    "from lineartree import LinearTreeRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, explained_variance_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f372efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        72091.0\n",
       "1       122994.0\n",
       "2       117865.0\n",
       "3       151282.0\n",
       "4       131881.0\n",
       "          ...   \n",
       "1479     46435.0\n",
       "1480     33636.0\n",
       "1481     29986.0\n",
       "1482     74748.0\n",
       "1483    240219.0\n",
       "Name: School-Aged Population, Length: 1484, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When first modelling, I had one notebook. I ran the first linear models against the original data and just the \n",
    "#school-aged population and school status columns. Below, I am running them with all my engineered features.\n",
    "\n",
    "merged_df_pre_omi = pd.read_csv('data/pre_omi.csv')\n",
    "\n",
    "merged_df_pre_omi['Test Date'] = pd.to_datetime(merged_df_pre_omi['Test Date'])\n",
    "\n",
    "merged_df_pre_omi['School Status'] = merged_df_pre_omi['School Status'].astype('object')\n",
    "merged_df_pre_omi['School-Aged Population']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b24ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Capita Gene Copies\n",
      "Population Served, estimated\n",
      "School-Aged Population\n"
     ]
    }
   ],
   "source": [
    "# Simple preprocessing. Not going to worry about scaling right now. Just attempting to determine if my data is\n",
    "# strong enough to warrant continuing.\n",
    "\n",
    "cat_cols = []\n",
    "\n",
    "for i in merged_df_pre_omi.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Test Date' or i == 'Holiday':\n",
    "        pass\n",
    "    elif merged_df_pre_omi[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif merged_df_pre_omi[i].dtype == 'float64' or 'int64':\n",
    "        print(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "\n",
    "\n",
    "cat_transformer = Pipeline(steps=[  \n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))                     \n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols),\n",
    "    ])\n",
    "\n",
    "y = merged_df_pre_omi['Gene Copies (N1/L)']\n",
    "\n",
    "X = merged_df_pre_omi.drop('Gene Copies (N1/L)', axis=1)\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "220b8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up columns to be onehot encoded. We will look at numerical transformation later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Have to drop the NaNs, otherwise linear model won't work\n",
    "y_train = y_train.dropna()\n",
    "X_train = X_train.dropna()\n",
    "X_test = X_test.dropna()\n",
    "y_test = y_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2575b6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6932029480436093"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred=linreg.predict(X_test)\n",
    "\n",
    "linreg.score(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29cea349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Date: -242.25819769575236\n",
      "Test Date: -1213.8680692830374\n",
      "WRRF Name: -450.36133955081175\n",
      "Per Capita Gene Copies: -88.97907742483167\n",
      "Population Served, estimated: -816.8782392129317\n",
      "School Status: -599.8194016551173\n",
      "School-Aged Population: 1037.239553925906\n",
      "Season: -2612.7220380119747\n",
      "Holiday: -1610.8580814343418\n"
     ]
    }
   ],
   "source": [
    "linear_model = linreg.named_steps['model']\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, coef in zip(X.columns, linear_model.coef_):\n",
    "    print(f\"{feature_name}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16865a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try again but with log-transformed targets\n",
    "y_train_2 = np.log(y_train)\n",
    "y_test_2 = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "661a9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7944152204682045"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_2 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_2.fit(X_train, y_train_2)\n",
    "\n",
    "y_pred_2=linreg_2.predict(X_test)\n",
    "\n",
    "linreg_2.score(X_train,y_train_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "035362af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Date: 0.021644905949258136\n",
      "Test Date: -0.95321807547912\n",
      "WRRF Name: -0.5068181306940807\n",
      "Per Capita Gene Copies: 0.19383371165303492\n",
      "Population Served, estimated: -0.038487768314116724\n",
      "School Status: -0.17055514483224704\n",
      "School-Aged Population: 0.65869233028895\n",
      "Season: -0.7364247507400248\n",
      "Holiday: -0.24013593876863606\n"
     ]
    }
   ],
   "source": [
    "linear_model = linreg_2.named_steps['model']\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, coef in zip(X.columns, linear_model.coef_):\n",
    "    print(f\"{feature_name}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce7f98d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the addition of jewish holidays, got a 1% boost in the r2 score. Might get a little nudge with islamic, but\n",
    "# probably not much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9de2ad18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sample Date', 'Test Date', 'WRRF Name', 'Gene Copies (N1/L)',\n",
       "       'Per Capita Gene Copies', 'Population Served, estimated',\n",
       "       'School Status', 'School-Aged Population', 'Season', 'Holiday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "836cb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying without the highly-correlated features\n",
    "copy_df = merged_df_pre_omi.copy()\n",
    "\n",
    "copy_df.dropna(inplace=True)\n",
    "\n",
    "y = copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "\n",
    "X = copy_df.drop(columns=['Gene Copies (N1/L)','Population Served, estimated', 'Per Capita Gene Copies'], axis=1)\n",
    "\n",
    "# Split again\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "# Log transforming the target\n",
    "\n",
    "y_test = np.log(y_test)\n",
    "y_train = np.log(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "359e13a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7975940643717525"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd iteration, no adjustments to the model itself\n",
    "\n",
    "linreg_3 = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_3.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_3.predict(X_test)\n",
    "\n",
    "linreg_3.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4eb1bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Date: -0.008693670534421008\n",
      "Test Date: -0.9950867642384545\n",
      "WRRF Name: -0.5303994956634813\n",
      "School Status: 0.47678370245459856\n",
      "School-Aged Population: -0.4366421136657676\n",
      "Season: 0.05649508602779869\n",
      "Holiday: 0.6610960179303406\n"
     ]
    }
   ],
   "source": [
    "linear_model = linreg_3.named_steps['model']\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, coef in zip(X.columns, linear_model.coef_):\n",
    "    print(f\"{feature_name}: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b70af9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly worse score without the non-school population data. So even though it's correlated, the general population\n",
    "# data is not boosting the score much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4773f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at just the school-related data.\n",
    "\n",
    "copy_df_2 = merged_df_pre_omi.copy()\n",
    "copy_df_2.dropna(inplace=True)\n",
    "\n",
    "y = copy_df_2['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_2[['School-Aged Population', 'School Status']]\n",
    "\n",
    "\n",
    "# We only have one column to transform in this version\n",
    "\n",
    "cat_cols_2 = ['School Status']\n",
    "\n",
    "preprocessor_2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols_2),\n",
    "    ])\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "05cb26cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46997170371988195"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4th model - this one is actually different!\n",
    "\n",
    "linreg_4 = Pipeline([\n",
    "    ('preprocessor', preprocessor_2),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_4.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_4.predict(X_test)\n",
    "\n",
    "linreg_4.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3ee16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so with just the school-aged population and school status, our model explains nearly half the variance! And this\n",
    "# is before log transformation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e2e9d4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5422711234817856\n",
      "0.5143954687950946\n"
     ]
    }
   ],
   "source": [
    "# Same model, log-transformed target data.\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "linreg_5 = Pipeline([\n",
    "    ('preprocessor', preprocessor_2),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_5.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_5.predict(X_test)\n",
    "\n",
    "print(linreg_5.score(X_train,y_train))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5bebc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, our train tests scores are pretty close! Not too much overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9cd54dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New X and y\n",
    "\n",
    "copy_df_3 = merged_df_pre_omi.copy()\n",
    "copy_df_3.dropna(inplace=True)\n",
    "y = copy_df_3['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_3['School-Aged Population']\n",
    "\n",
    "\n",
    "# Split again\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# log transform again\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6996df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:     Gene Copies (N1/L)   R-squared:                       0.007\n",
      "Model:                            OLS   Adj. R-squared:                  0.007\n",
      "Method:                 Least Squares   F-statistic:                     8.238\n",
      "Date:                Mon, 31 Jul 2023   Prob (F-statistic):            0.00418\n",
      "Time:                        10:58:54   Log-Likelihood:                -1784.3\n",
      "No. Observations:                1097   AIC:                             3573.\n",
      "Df Residuals:                    1095   BIC:                             3583.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                      8.1025      0.074    108.769      0.000       7.956       8.249\n",
      "School-Aged Population  1.787e-06   6.23e-07      2.870      0.004    5.66e-07    3.01e-06\n",
      "==============================================================================\n",
      "Omnibus:                       63.016   Durbin-Watson:                   2.036\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               39.333\n",
      "Skew:                          -0.331   Prob(JB):                     2.88e-09\n",
      "Kurtosis:                       2.350   Cond. No.                     2.40e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.4e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "602     1.749047\n",
      "313    -0.568979\n",
      "1420   -0.871085\n",
      "737     0.765110\n",
      "818    -0.498675\n",
      "          ...   \n",
      "1198    1.493369\n",
      "1238   -0.735291\n",
      "1405    0.034741\n",
      "934    -0.292939\n",
      "1234   -1.714324\n",
      "Length: 1097, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# using ols because we are not preprocessing this data\n",
    "\n",
    "\n",
    "X_int = sm.add_constant(X_train)\n",
    "results = sm.OLS(y_train, X_int).fit()\n",
    "summary = results.summary()\n",
    "print(summary)\n",
    "\n",
    "influence = OLSInfluence(results)\n",
    "print(influence.resid_studentized)\n",
    "\n",
    "# well, this isn't very good! This is just school-aged population, though, and not the actual status of schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "201df553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wonder how predictive the baseline model is. Let's just look at the original population data, which is one of\n",
    "# the most relevant features. \n",
    "\n",
    "copy_df_4 = merged_df_pre_omi.copy()\n",
    "copy_df_4.dropna(inplace=True)\n",
    "\n",
    "\n",
    "y = copy_df_4['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_4['Population Served, estimated']\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b0bd2bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>Gene Copies (N1/L)</td> <th>  R-squared:         </th> <td>   0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                    <td>OLS</td>        <th>  Adj. R-squared:    </th> <td>   0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>              <td>Least Squares</td>   <th>  F-statistic:       </th> <td>   11.33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Mon, 31 Jul 2023</td>  <th>  Prob (F-statistic):</th> <td>0.000791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>10:58:56</td>      <th>  Log-Likelihood:    </th> <td> -1782.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>  1097</td>       <th>  AIC:               </th> <td>   3569.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>  1095</td>       <th>  BIC:               </th> <td>   3579.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>       <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                        <td>    8.0670</td> <td>    0.075</td> <td>  106.995</td> <td> 0.000</td> <td>    7.919</td> <td>    8.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Population Served, estimated</th> <td> 3.618e-07</td> <td> 1.08e-07</td> <td>    3.365</td> <td> 0.001</td> <td> 1.51e-07</td> <td> 5.73e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>61.143</td> <th>  Durbin-Watson:     </th> <td>   2.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  38.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.330</td> <th>  Prob(JB):          </th> <td>3.98e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.359</td> <th>  Cond. No.          </th> <td>1.42e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.42e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:     Gene Copies (N1/L)   R-squared:                       0.010\n",
       "Model:                            OLS   Adj. R-squared:                  0.009\n",
       "Method:                 Least Squares   F-statistic:                     11.33\n",
       "Date:                Mon, 31 Jul 2023   Prob (F-statistic):           0.000791\n",
       "Time:                        10:58:56   Log-Likelihood:                -1782.7\n",
       "No. Observations:                1097   AIC:                             3569.\n",
       "Df Residuals:                    1095   BIC:                             3579.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================================\n",
       "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------\n",
       "const                            8.0670      0.075    106.995      0.000       7.919       8.215\n",
       "Population Served, estimated  3.618e-07   1.08e-07      3.365      0.001    1.51e-07    5.73e-07\n",
       "==============================================================================\n",
       "Omnibus:                       61.143   Durbin-Watson:                   2.034\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               38.686\n",
       "Skew:                          -0.330   Prob(JB):                     3.98e-09\n",
       "Kurtosis:                       2.359   Cond. No.                     1.42e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.42e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_int = sm.add_constant(X_train)\n",
    "model_2 = sm.OLS(y_train, X_int).fit()\n",
    "summary = model_2.summary()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a09e687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df_5 = merged_df_pre_omi.copy()\n",
    "copy_df_5.dropna(inplace=True)\n",
    "y = copy_df_5['Gene Copies (N1/L)']\n",
    "\n",
    "X = copy_df_5[['Population Served, estimated', 'WRRF Name', 'Per Capita Gene Copies', 'Test Date']]\n",
    "\n",
    "X_train ,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa973bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07123753415992828\n",
      "0.04505102157812135\n"
     ]
    }
   ],
   "source": [
    "cat_cols_3 = ['WRRF Name']\n",
    "\n",
    "cat_transformer = Pipeline(steps=[  \n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))                     \n",
    "])\n",
    "\n",
    "preprocessor_3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols_3),\n",
    "    ])\n",
    "\n",
    "linreg_6 = Pipeline([\n",
    "    ('preprocessor', preprocessor_3),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "linreg_6.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linreg_6.predict(X_test)\n",
    "\n",
    "print(linreg_6.score(X_train,y_train))\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36968087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So yeah, the basic info from the original dataset explains almost nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8d39e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to use the index (sample date) as a feature. Let's also save this data file as a csv, since\n",
    "# it's what we ultimately want to use.\n",
    "\n",
    "sample_date_df = merged_df_pre_omi.copy()\n",
    "sample_date_df['Sample Date'] = pd.to_datetime(sample_date_df['Sample Date'])\n",
    "\n",
    "# sample_date_df.to_csv('data/master_wastewater.csv', index=False)\n",
    "\n",
    "#(Ending up abandoning dates at features, but in future work would like to integrate them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f040b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sample Date', 'Test Date']\n",
      "['WRRF Name', 'School Status', 'Season']\n",
      "['Per Capita Gene Copies', 'Population Served, estimated', 'School-Aged Population']\n"
     ]
    }
   ],
   "source": [
    "# Fancier models! Going to move on to more elaborate models, including Random Forest.\n",
    "\n",
    "# Re-doing the columns for preprocessing since we have different features\n",
    "\n",
    "date_cols = []\n",
    "cat_cols = []  \n",
    "num_cols = []   \n",
    "\n",
    "for i in sample_date_df.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Holiday':\n",
    "        pass\n",
    "    elif sample_date_df[i].dtype == 'datetime64[ns]':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif i == 'Sample Date':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'float64' or sample_date_df[i].dtype == 'int64':\n",
    "        num_cols.append(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "\n",
    "\n",
    "print(date_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)\n",
    "# leaving \"holiday\" out because we don't want to transform this binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8cd2084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for fit and transformation of linear features in a pipeline. Were originally trying to combine\n",
    "# LinearRegressor with RandomForest using FeatureUnion, but was unsuccessful\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.columns]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "173f31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New X and y, split\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop('Gene Copies (N1/L)', axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be5fade7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sample Date', 'Test Date', 'WRRF Name', 'Gene Copies (N1/L)',\n",
       "       'Per Capita Gene Copies', 'Population Served, estimated',\n",
       "       'School Status', 'School-Aged Population', 'Season', 'Holiday'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_copy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4a6da72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New preprocessors\n",
    "\n",
    "cat_preprocessor = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_preprocessor = Pipeline([\n",
    "    ('selector', ColumnSelector(columns=num_cols)), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_cols),\n",
    "    ('num', numeric_preprocessor, num_cols),\n",
    "])\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  RandomForestRegressor(random_state = 42))\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "967c50c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.9951082673198972\n",
      "R2 test score: 0.9675638498965449\n",
      "Mean squared error: 0.052094098364203714\n",
      "Mean absolute error: 0.15678014488775047\n",
      "Mean absolute percentage error: 0.019586285838186914\n",
      "Explained variance score (modified R2): 0.9675841799049261\n"
     ]
    }
   ],
   "source": [
    "# Just going with a vanilla Random Forest model after trying to combine a linear regression and random forest.\n",
    "\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {rf_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f82ddf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow, 99.5%! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc5f936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033534293203989632: Sample Date\n",
      "0.0013350698361288351: Holiday\n",
      "0.0013342269200147376: Test Date\n",
      "0.001027440180672196: School-Aged Population\n",
      "0.00101529776794821: Season\n",
      "0.0008777949385026092: Population Served, estimated\n",
      "0.0006330406638788463: Per Capita Gene Copies\n",
      "0.00044756918311024147: WRRF Name\n",
      "0.00036113194992653256: School Status\n"
     ]
    }
   ],
   "source": [
    "# Let's look at feature importances:\n",
    "\n",
    "rf_model = rf_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, rf_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1ade2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.81501698, 0.80086398, 0.79063416, 0.79651999, 0.81004596]),\n",
       " 'score_time': array([0.01636505, 0.01608706, 0.01679897, 0.01606894, 0.01622081]),\n",
       " 'test_score': array([0.97273797, 0.96571812, 0.96220805, 0.96419641, 0.95084884]),\n",
       " 'train_score': array([0.99458214, 0.99494814, 0.99498579, 0.99466468, 0.99549897])}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validating, because we ultimately want to use this on other data!\n",
    "cross_validate(rf_pipeline, X_train, y_train, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "be4f4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to try this model again without the date features, since they seem to be over-determining.\n",
    "\n",
    "sample_copy_df_2 = sample_date_df.copy()\n",
    "\n",
    "sample_copy_df_2.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df_2.drop(['Gene Copies (N1/L)', 'Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df_2['Gene Copies (N1/L)']\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59823e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = []\n",
    "cat_cols = []  \n",
    "num_cols = []   \n",
    "\n",
    "for i in sample_date_df.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Holiday':\n",
    "        pass\n",
    "    elif sample_date_df[i].dtype == 'datetime64[ns]':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif i == 'Sample Date':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'float64' or sample_date_df[i].dtype == 'int64':\n",
    "        num_cols.append(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "\n",
    "\n",
    "print(date_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "04dfa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_preprocessor = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_preprocessor = Pipeline([\n",
    "    ('selector', ColumnSelector(columns=num_cols)), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_cols),\n",
    "    ('num', numeric_preprocessor, num_cols),\n",
    "])\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  RandomForestRegressor(random_state = 42))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "88f8691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.9951082673198972\n",
      "R2 test score: 0.9675638498965449\n",
      "Mean squared error: 0.052094098364203714\n",
      "Mean absolute error: 0.15678014488775047\n",
      "Mean absolute percentage error: 0.019586285838186914\n",
      "Explained variance score (modified R2): 0.9675841799049261\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline.fit(X_train, y_train)  # Same pipeline, different data\n",
    "\n",
    "\n",
    "y_pred = rf_pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "print(f'R-squared score on training data: {rf_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3843f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033534293203989632: WRRF Name\n",
      "0.0013342269200147376: Per Capita Gene Copies\n",
      "0.001027440180672196: Holiday\n",
      "0.0008777949385026092: School-Aged Population\n",
      "0.0006330406638788463: School Status\n",
      "0.00044756918311024147: Population Served, estimated\n",
      "0.00036113194992653256: Season\n"
     ]
    }
   ],
   "source": [
    "# The very same! Our engineered features are very strong all around. Let's look at feature importance.\n",
    "\n",
    "rf_model = rf_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, rf_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a36730e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try another model type, using all features\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "X = sample_copy_df.drop('Gene Copies (N1/L)', axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "y.dropna(inplace=True)\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3e83afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.9814959729055575\n",
      "R2 test score: 0.9723183570900568\n",
      "Mean squared error: 0.04445811922912941\n",
      "Mean absolute error: 0.14413801082084243\n",
      "Mean absolute percentage error: 0.01798534397514475\n",
      "Explained variance score (modified R2 test score): 0.9723229599966545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grad = GradientBoostingRegressor()\n",
    "grad_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  grad)\n",
    "])\n",
    "\n",
    "\n",
    "grad_pipeline.fit(X_train, y_train)\n",
    "y_pred = grad_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {grad_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0d3c73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ae966026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0039642306151674404: Sample Date\n",
      "0.0018551912240568251: Holiday\n",
      "0.0010896138197174696: School-Aged Population\n",
      "0.0007320431865119407: Season\n",
      "0.0006780813323552669: Population Served, estimated\n",
      "0.0004589845674212528: Test Date\n",
      "0.0003816646083538947: Per Capita Gene Copies\n",
      "1.2022278342754647e-05: School Status\n",
      "5.498453807413116e-06: WRRF Name\n"
     ]
    }
   ],
   "source": [
    "grad_model = grad_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, grad_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d189dfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2546271 , 1.0343755 , 0.85524127, 0.70958707, 0.59109145,\n",
       "       0.4945883 , 0.41586988, 0.35165381, 0.29935438, 0.25651436,\n",
       "       0.221891  , 0.19355449, 0.17034788, 0.15123911, 0.13525078,\n",
       "       0.12200857, 0.11088258, 0.10184361, 0.09408602, 0.08763019,\n",
       "       0.08227315, 0.07697006, 0.07258206, 0.0690886 , 0.06602022,\n",
       "       0.06324645, 0.06088238, 0.05882038, 0.05690847, 0.05521488,\n",
       "       0.05357605, 0.05218039, 0.05038489, 0.04914069, 0.04801434,\n",
       "       0.04685401, 0.04577433, 0.04487213, 0.04399308, 0.04311993,\n",
       "       0.0423678 , 0.04161209, 0.04089222, 0.03969664, 0.03909103,\n",
       "       0.03846022, 0.03794208, 0.03745044, 0.03702689, 0.03632175,\n",
       "       0.03593234, 0.03559401, 0.03521393, 0.03492283, 0.03463449,\n",
       "       0.0343394 , 0.03404891, 0.03374259, 0.03347791, 0.03326131,\n",
       "       0.03307398, 0.03287956, 0.03270017, 0.0324756 , 0.03231803,\n",
       "       0.03210624, 0.03182856, 0.03168736, 0.03153384, 0.03138392,\n",
       "       0.0312248 , 0.03109716, 0.03097769, 0.03087304, 0.03076456,\n",
       "       0.03065913, 0.03054184, 0.03046266, 0.03038044, 0.03030521,\n",
       "       0.03020819, 0.03011904, 0.0300473 , 0.02999498, 0.02966244,\n",
       "       0.02958765, 0.02952096, 0.02946424, 0.02940316, 0.02919121,\n",
       "       0.02892181, 0.02885622, 0.02880944, 0.0287618 , 0.02857185,\n",
       "       0.02852567, 0.02843904, 0.02832453, 0.02828111, 0.02823634])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very different weighing of features here, and slightly better test score. Let's look at loss scores, too:\n",
    "train_score = grad_pipeline['model'].train_score_\n",
    "train_score\n",
    "\n",
    "\n",
    "# If we ran maybe double the iteratations, our loss score should approach .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "61753617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.9814959729055575\n",
      "R2 test score: 0.972319965822545\n",
      "Mean squared error: 0.04445553552334996\n",
      "Mean absolute error: 0.14412556858772274\n",
      "Mean absolute percentage error: 0.017984162569847115\n",
      "Explained variance score (modified R2 test score): 0.9723245266981202\n"
     ]
    }
   ],
   "source": [
    "# Let's get rid of all date features with this model and see what happens\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "y.dropna(inplace=True)\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "grad = GradientBoostingRegressor()\n",
    "grad_best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  grad)\n",
    "])\n",
    "\n",
    "grad_best_pipeline.fit(X_train, y_train)\n",
    "y_pred = grad_best_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {grad_best_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "135bd58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003964230615167441: WRRF Name\n",
      "0.0010896138197174707: Holiday\n",
      "0.0006791720086939195: School-Aged Population\n",
      "0.0004589845674212526: Per Capita Gene Copies\n",
      "0.0003816646083538939: School Status\n",
      "1.1101917699992118e-05: Season\n",
      "5.498453807413116e-06: Population Served, estimated\n"
     ]
    }
   ],
   "source": [
    "# Very similar scores, with less over-fitting. Good! # Let's examine feature importance.\n",
    "\n",
    "grad_model = grad_best_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, grad_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4559fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to remove highly-correlated non-engineered features, to better see how strong the model is with my contibutions.\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date', 'Population Served, estimated', 'Per Capita Gene Copies'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1383b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sample Date', 'Test Date']\n",
      "['WRRF Name', 'School Status', 'Season']\n",
      "['School-Aged Population']\n"
     ]
    }
   ],
   "source": [
    "# Re-do preprocesing as we have different features\n",
    "date_cols = []\n",
    "cat_cols = []  \n",
    "num_cols = []   \n",
    "\n",
    "for i in sample_date_df.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Holiday' or i == 'Population Served, estimated' or i == 'Per Capita Gene Copies':\n",
    "        pass\n",
    "    elif sample_date_df[i].dtype == 'datetime64[ns]':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif i == 'Sample Date':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'float64' or sample_date_df[i].dtype == 'int64':\n",
    "        num_cols.append(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "\n",
    "\n",
    "print(date_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)\n",
    "# leaving \"holiday\" out because we don't want to transform this binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a0fef3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.6596607521088087\n",
      "R2 test score: 0.6154788665536127\n",
      "Mean squared error: 0.6175603974263713\n",
      "Mean absolute error: 0.6082857308635666\n",
      "Mean absolute percentage error: 0.07673133747053865\n",
      "Explained variance score (modified R2 test score): 0.6176639745109003\n"
     ]
    }
   ],
   "source": [
    "cat_transformer = Pipeline(steps=[  \n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore'))                     \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_transformer, cat_cols),\n",
    "    ])\n",
    "\n",
    "grad = GradientBoostingRegressor()\n",
    "grad_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model',  grad)\n",
    "])\n",
    "\n",
    "grad_pipeline.fit(X_train, y_train)\n",
    "y_pred = grad_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {grad_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8f29db9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01684000976720728: School-Aged Population\n",
      "0.007501852048518024: Holiday\n",
      "0.005086662556497535: School Status\n",
      "0.0019261258027081182: WRRF Name\n",
      "0.0015815597356382753: Season\n"
     ]
    }
   ],
   "source": [
    "# Looking at feature importances\n",
    "\n",
    "grad_model = grad_pipeline.named_steps['model']\n",
    "features = []\n",
    "scores = []\n",
    "# Print the coefficients along with column names\n",
    "for feature_name, importance in zip(X.columns, grad_model.feature_importances_):\n",
    "    features.append(feature_name)\n",
    "    scores.append(importance)\n",
    "    \n",
    "ranked_scores = sorted(zip(scores, features), reverse=True)\n",
    "for score, feature in ranked_scores:\n",
    "    print(f'{score}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fb2ba988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By removing correlated features, our score was reduced pretty dramatically. However, there seem to be important\n",
    "# elements of the non-date correlated features that we should keep, because although they are similar, they tell us\n",
    "# important things about the data at particular times. For instance, sudden wastewater spikes around school events\n",
    "# in places with high school-aged populations. Over time, these effects are expected to flatten out and/or become\n",
    "# more cyclical. One avenue to pursue later would be population estimates of those who left the city during 2020/2021\n",
    "# but who were never officially non-residents, and thus not reflected in the Census Bureau's data. We would expect much more of this in wealthier zip codes and in zip codes\n",
    "# where there are fewer children. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c6992721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sample Date', 'Test Date']\n",
      "['WRRF Name', 'School Status', 'Season']\n",
      "['Per Capita Gene Copies', 'Population Served, estimated', 'School-Aged Population']\n"
     ]
    }
   ],
   "source": [
    "# Setting up for grid search of our best model so far. Re-doing preprocessing since using different features\n",
    "\n",
    "date_cols = []\n",
    "cat_cols = []  \n",
    "num_cols = []   \n",
    "\n",
    "for i in sample_date_df.columns:\n",
    "    if i == 'Gene Copies (N1/L)' or i == 'Holiday':\n",
    "        pass\n",
    "    elif sample_date_df[i].dtype == 'datetime64[ns]':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    elif i == 'Sample Date':\n",
    "        date_cols.append(i)\n",
    "    elif sample_date_df[i].dtype == 'float64' or sample_date_df[i].dtype == 'int64':\n",
    "        num_cols.append(i)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        \n",
    "print(date_cols)\n",
    "print(cat_cols)\n",
    "print(num_cols)\n",
    "\n",
    "cat_preprocessor = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_preprocessor = Pipeline([\n",
    "    ('selector', ColumnSelector(columns=num_cols)), \n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_cols),\n",
    "    ('num', numeric_preprocessor, num_cols),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "200ddb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation score: 0.9728682096542398\n",
      "Test score: 0.9723028481450525\n",
      "{'model__criterion': 'friedman_mse', 'model__learning_rate': 0.1, 'model__loss': 'absolute_error', 'model__n_estimators': 250, 'model__subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Removing the datetime info and re-running grid search on our best model.\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "grad = GradientBoostingRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'model__learning_rate': [.01, .1, .3],\n",
    "    'model__loss': ['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "    'model__n_estimators': [100, 150, 200, 250],\n",
    "    'model__subsample': [.3, .5, .7, 1.0],\n",
    "    'model__criterion': ['friedman_mse', 'squared_error']\n",
    "    \n",
    "}\n",
    "                            \n",
    "grid_search_gbc = GridSearchCV(\n",
    "    estimator = grad_best_pipeline,  # pipeline \n",
    "    param_grid = param_grid,\n",
    "    cv= 5,\n",
    "    scoring='explained_variance'  # internal scoring term\n",
    ")\n",
    "\n",
    "grid_search_gbc.fit(X_train, y_train)\n",
    "\n",
    "cv_score = grid_search_gbc.best_score_\n",
    "test_score = r2_score(y_test, grid_search_gbc.predict(X_test))\n",
    "\n",
    "print(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\n",
    "print(grid_search_gbc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7b5927df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thought I was done, but want to try one more model type: LinearTrees!\n",
    "\n",
    "sample_copy_df = sample_date_df.copy()\n",
    "\n",
    "sample_copy_df.dropna(inplace=True)\n",
    "\n",
    "X = sample_copy_df.drop(columns=['Gene Copies (N1/L)','Sample Date', 'Test Date'], axis=1)\n",
    "\n",
    "y = sample_copy_df['Gene Copies (N1/L)']\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b18a8d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score on training data: 0.9802052054633759\n",
      "R2 test score: -1184.0720230715035\n",
      "Mean squared error: 1903.285634751599\n",
      "Mean absolute error: 3.5056331972161354\n",
      "Mean absolute percentage error: 0.4220202108653636\n",
      "Explained variance score (modified R2 test score): -1177.0278641925847\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lintree = LinearTreeRegressor(base_estimator=LinearRegression())\n",
    "\n",
    "lintree_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('to_sparse', FunctionTransformer(csr_matrix, validate=False)),  # convert to sparse\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.toarray(), validate=False)), # convert to dense. lintree requires this\n",
    "    ('model',  lintree)\n",
    "])\n",
    "\n",
    "lintree_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lintree_pipeline.predict(X_test)\n",
    "\n",
    "print(f'R-squared score on training data: {lintree_pipeline.score(X_train, y_train)}')\n",
    "print(f'R2 test score: {r2_score(y_test, y_pred)}')\n",
    "print(f'Mean squared error: {mean_squared_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute error: {mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'Mean absolute percentage error: {mean_absolute_percentage_error(y_test, y_pred)}')\n",
    "print(f'Explained variance score (modified R2 test score): {explained_variance_score(y_test, y_pred)}')\n",
    "\n",
    "# Well, that was a terrible model for this data, but seemed interesting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aae6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MCMPrime)",
   "language": "python",
   "name": "mcmprime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
